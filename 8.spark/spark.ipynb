{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89fddda3-9e2a-41d3-8549-9b9e91b90e74",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "Universidad Nacional de Colombia <br>\n",
    "Maestría en Ciencias - Estadística <br>\n",
    "Big Data - Métodos Intensivos de Computación Estadística -2023829 <br>\n",
    "Semestre 2021-1 \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460457d1-d071-489d-961b-eb69a1178c2e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> \n",
    "Estudiante: Diego Clemente Rojas Chingate   <br>\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d4827-c6de-4abb-afe0-351b6c68a04b",
   "metadata": {},
   "source": [
    "<img src=\"img/spark_logo.png\" align=\"right\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4837d-fe87-4b7d-9b14-194d3811676e",
   "metadata": {},
   "source": [
    "# Tarea: Reproducir el cuaderno [spark.ipynb](https://github.com/AprendizajeProfundo/BigData/blob/main/Spark/Cuadernos/Spark_install.ipynb) localmente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20dd17-2f32-4883-a1b9-dab165f0f758",
   "metadata": {},
   "source": [
    "## Instalar prerrequisitos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e032142-aef6-4fbb-ae60-3c7273c49622",
   "metadata": {},
   "source": [
    "[drojas@localhost ~]$ sudo yum install java-11-openjdk java-11-openjdk-devel git\n",
    "[drojas@localhost ~]$ cd ~java-11-openjdk\n",
    "[drojas@localhost ~]$ wget http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.rpm\n",
    "[drojas@localhost ~]$ sudo yum install scala-2.11.8.rpm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e2c8f9c-005b-43b2-af09-15e7538a76e7",
   "metadata": {},
   "source": [
    "[drojas@localhost ~]$ java -version; javac -version; scala -version; git --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3dfdb-1882-47b0-a015-79218718a6d5",
   "metadata": {},
   "source": [
    "openjdk version \"1.8.0_275\"  \n",
    "OpenJDK Runtime Environment (build 1.8.0_275-b01)  \n",
    "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)  \n",
    "javac 1.8.0_275  \n",
    "Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL  \n",
    "git version 2.31.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e8a49-8925-4f40-8863-6b1e4382d1e8",
   "metadata": {},
   "source": [
    "## Instalar Spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "030a6a82-5853-4333-9040-05d439218ed2",
   "metadata": {},
   "source": [
    "[drojas@localhost ~]$ wget https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
    "[drojas@localhost ~]$ tar xvf spark-*\n",
    "[drojas@localhost ~]$ sudo mv spark-3.1.2-bin-hadoop3.2 /opt/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da4eecd-47bb-4e25-9862-3253253be06f",
   "metadata": {},
   "source": [
    "## Configurar el entorno de Spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba83cecf-098d-4d4e-bf70-5cc2d108f386",
   "metadata": {},
   "source": [
    "[drojas@localhost ~]$ echo \"export SPARK_HOME=/opt/spark\" >> ~/.bash_profile\n",
    "[drojas@localhost ~]$ echo \"export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\" >> ~/.bash_profile\n",
    "[drojas@localhost ~]$ echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.bash_profile\n",
    "[drojas@localhost ~]$ source ~/.bash_profile "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e703c63-3600-49ee-88ca-990850fa9a71",
   "metadata": {},
   "source": [
    "## Iniciar el servidor maestro autónomo de Spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "641350ad-0680-4852-b74d-7bb88acd1333",
   "metadata": {},
   "source": [
    "[drojas@localhost ~]$ start-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c8f3d-7dff-4740-a185-2c4fa93fe8c1",
   "metadata": {},
   "source": [
    "<img src=\"img/master.png\" align=\"center\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeea8be-6d8b-4f4e-a413-8f110709aeec",
   "metadata": {},
   "source": [
    "## Iniciar Spark Slave Server (Iniciar un proceso de trabajo)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "553e2a26-825b-47eb-98ae-72aa140b7f0f",
   "metadata": {},
   "source": [
    "[drojas@localhost ~]$ start-slave.sh spark://localhost:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf278aab-f612-4667-9fcb-17c50d0edd4f",
   "metadata": {},
   "source": [
    "<img src=\"img/slave.png\" align=\"center\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a58a19-c2eb-4ca5-84e9-b47ed4f7bdc8",
   "metadata": {},
   "source": [
    "## Shell de Spark (Interfaz de Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51784e-b3df-4abd-b758-b945592937d0",
   "metadata": {},
   "source": [
    "<img src=\"img/shell.png\" align=\"center\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf647b-97cb-48ac-bb0c-55e82aa9fddd",
   "metadata": {},
   "source": [
    "## Python en Spark: PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4301ddf-de8f-4645-b339-44bf34fd4b60",
   "metadata": {},
   "source": [
    "<img src=\"img/pyspark.png\" align=\"center\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85d341-98d7-4b7c-828a-d2ccd1f01ab9",
   "metadata": {},
   "source": [
    "## Comandos básicos para iniciar y detener el servidor maestro y los trabajadores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a344bbf-8cc6-49e4-b435-5e992b5f1084",
   "metadata": {},
   "source": [
    "A continuación, se muestran los comandos básicos para iniciar y detener el `servidor maestro Apache Spark` y los trabajadores. Dado que esta configuración es solo para una máquina, las secuencias de comandos que ejecuta se establecen de forma predeterminada en el host local.\n",
    "\n",
    "Para iniciar una instancia de servidor maestro en la máquina actual, ejecute el comando que usamos anteriormente en este tutoria. \n",
    "\n",
    "- `start-master.sh`\n",
    "\n",
    "Para pararlo use \n",
    "\n",
    "- `stop-master.sh`\n",
    "\n",
    "Para lanzar y detener instancias de workers aosicadoa a su masters sue respectivamente\n",
    "\n",
    "- `start-slave.sh`\n",
    "- `stop-slave.sh`\n",
    "\n",
    "Finalmente¿, para lanzar y detener instancias  tanto de master como slave escriba\n",
    "\n",
    "-  `start-all.sh`\n",
    "- `stop-all.sh`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc5ea3-ec86-4f95-87e5-a20c2c83f73f",
   "metadata": {},
   "source": [
    "# Instalación  para correr Spark desde Jupyterlab"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0f01cb9-58ec-4c0d-8efb-fe8b9cce976e",
   "metadata": {},
   "source": [
    "[drojas@localhost ~]$ conda create -n spark python=3.9\n",
    "[drojas@localhost ~]$ conda activate spark\n",
    "(spark) [drojas@localhost ~]$ conda install -c conda-forge findspark\n",
    "(spark) [drojas@localhost ~]$ conda install -c conda-forge pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5423ce84-1383-48e8-aa61-0aed793e7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24311b6-2ffa-4cfa-bad1-012b917290c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1416852\n"
     ]
    }
   ],
   "source": [
    "# crea un contexto Spark\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "\n",
    "# genera 100 millones muestras para estimar $\\pi$.\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "# detiene el contexto\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30c549-4e53-4e68-97f8-1c5027b1a9b4",
   "metadata": {},
   "source": [
    "## Spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5afafcec-b2b9-4d77-b3bf-08e5194e12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('myFirstSparkSession').getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./datos/employee_data.csv\",header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be5ba40-13b4-4785-bf46-3930bc602215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "|       M002|         Bill| 64|Washington|   10|\n",
      "|       A003|         Jeff| 56|Washington|   11|\n",
      "|       A004|         Cook| 59|California|   12|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "430bd06e-f362-4c91-94a5-990fbd6dcf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- hours: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee3b1b9-7820-44f4-bd0e-67debd91e675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- hours: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./datos/employee_data.csv\",header=True,inferSchema=True) \n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8fa52d0-0b5d-49f3-aeb8-44fe0a82e679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['employee_id', 'employee_name', 'age', 'location', 'hours']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd95fe-2dd1-4227-b50e-b6c75a77f54c",
   "metadata": {},
   "source": [
    "## Trabajando con columnas en Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387840d3-2ad1-48b9-93df-71e760a86589",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-176e0fb11baa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df['age'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44e84e5-ba26-41f2-9fba-9134456a1310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 47|\n",
      "| 64|\n",
      "| 56|\n",
      "| 59|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b08a44b0-5b0b-40fb-b5e5-9a92ac6232d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|employee_name|age|\n",
      "+-------------+---+\n",
      "|       Pichai| 47|\n",
      "|         Bill| 64|\n",
      "|         Jeff| 56|\n",
      "|         Cook| 59|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['employee_name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d68549-ea34-4554-b2ea-fa1d3c83e1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|employee_name|age|\n",
      "+-------------+---+\n",
      "|       Pichai| 47|\n",
      "|         Bill| 64|\n",
      "|         Jeff| 56|\n",
      "|         Cook| 59|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['employee_name', 'age'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db962d3-6ce5-4f9c-845a-6a499b1ba6b9",
   "metadata": {},
   "source": [
    "## Nuevas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01ceeecb-6d6c-4f74-9c94-39f1ce43edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+----------+-----+------------+\n",
      "|employee_id|employee_name|age|  location|hours|horas_extras|\n",
      "+-----------+-------------+---+----------+-----+------------+\n",
      "|       G001|       Pichai| 47|California|   14|          14|\n",
      "|       M002|         Bill| 64|Washington|   10|          10|\n",
      "|       A003|         Jeff| 56|Washington|   11|          11|\n",
      "|       A004|         Cook| 59|California|   12|          12|\n",
      "+-----------+-------------+---+----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df.withColumn('horas_extras', df['hours'])\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f005e2a-34c3-4414-846d-645772416449",
   "metadata": {},
   "source": [
    "## Renombrar columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff09f86a-2633-42cc-95df-232b201c9e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|horas|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "|       M002|         Bill| 64|Washington|   10|\n",
      "|       A003|         Jeff| 56|Washington|   11|\n",
      "|       A004|         Cook| 59|California|   12|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed = df.withColumnRenamed('hours', 'horas')\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373512e3-4692-4209-bea6-54a64d62bcda",
   "metadata": {},
   "source": [
    "## Eliminar una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "211c4042-06cf-4a19-af89-55a44dfb2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+----------+------------+\n",
      "|employee_id|employee_name|age|  location|horas_extras|\n",
      "+-----------+-------------+---+----------+------------+\n",
      "|       G001|       Pichai| 47|California|          14|\n",
      "|       M002|         Bill| 64|Washington|          10|\n",
      "|       A003|         Jeff| 56|Washington|          11|\n",
      "|       A004|         Cook| 59|California|          12|\n",
      "+-----------+-------------+---+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.drop('hours')\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a7077-38f7-4c0b-a5e5-af9216348ebf",
   "metadata": {},
   "source": [
    "## Cálculo de estadísticas globales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42a2b295-bd15-4d38-b822-8219cb34fe85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(employee_id='G001', employee_name='Pichai', age=47, working_hours=14, productive_hours=11),\n",
       " Row(employee_id='M002', employee_name='Bill', age=64, working_hours=10, productive_hours=7),\n",
       " Row(employee_id='A003', employee_name='Jeff', age=56, working_hours=11, productive_hours=8)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df.withColumn(\"productive_hours\",df[\"hours\"]-3)\n",
    "df_renamed = df_new.withColumnRenamed(\"hours\",\"working_hours\")\n",
    "df_final = df_renamed.drop(\"location\") \n",
    "df_final.head(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00b6b006-6a82-49d5-bed5-18fe26200b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Age, Working hours and Productive hours\n",
      "+-------+----------------+-----------------+-----------------+\n",
      "|summary|             age|    working_hours| productive_hours|\n",
      "+-------+----------------+-----------------+-----------------+\n",
      "|  count|               4|                4|                4|\n",
      "|   mean|            56.5|            11.75|             8.75|\n",
      "| stddev|7.14142842854285|1.707825127659933|1.707825127659933|\n",
      "|    min|              47|               10|                7|\n",
      "|    max|              64|               14|               11|\n",
      "+-------+----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSummary of Age, Working hours and Productive hours\")\n",
    "summary_data = df_final.select([\"age\",\"working_hours\",\"productive_hours\"])\n",
    "summary_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa7479-6394-4285-9d6c-ba8d89f2fe3a",
   "metadata": {},
   "source": [
    "## SQL con los dataframes de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e25142e4-9282-45a0-961b-0c20405f9307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "|       M002|         Bill| 64|Washington|   10|\n",
      "|       A003|         Jeff| 56|Washington|   11|\n",
      "|       A004|         Cook| 59|California|   12|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./datos/employee_data.csv\",header=True,inferSchema=True)\n",
    "\n",
    "df.createOrReplaceTempView(\"associates\") \n",
    "\n",
    "sql_result_1 = spark.sql(\"SELECT * FROM associates\") \n",
    "\n",
    "sql_result_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98c864d1-82b2-43e0-9fe8-e32019bba026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "|       A004|         Cook| 59|California|   12|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result_2 = spark.sql(\"SELECT * FROM associates WHERE age BETWEEN 45 AND 60 AND location='California'\")\n",
    "sql_result_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517fa46-4c4a-4c3a-972f-7510bd043ec6",
   "metadata": {},
   "source": [
    "## Operaciones y funciones con los dataframes de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e11f3a89-ad53-4c47-824c-e8284818dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|11-10-2018|     Beer|     110.5|       2|     53.04|      163.54|\n",
      "|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "|05-10-2018|      Rum|     550.0|       2|     264.0|       814.0|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkFilters\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./datos/items_bought.csv\",header=True,inferSchema=True)\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79addc19-e7dc-44ef-9729-2a55a0958dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"total_amount>1500\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a949734-f9a7-4e2d-97aa-cf6da95da6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"item_price\"]>1000)&(df[\"tax_amount\"]>500)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "404a34e3-e944-4e98-a91f-f6caec6a83a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collected data point's date value is 23-03-2020\n"
     ]
    }
   ],
   "source": [
    "result_data = df.filter((df[\"total_amount\"]==1924.74)).collect()\n",
    "resulting_date = result_data[0][\"date\"] \n",
    "print(\"The collected data point's date value is \" + resulting_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6eb93412-08e2-4143-8846-d434918f4097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '23-03-2020',\n",
       " 'item_name': 'Whisky',\n",
       " 'item_price': 1300.5,\n",
       " 'quantity': 2,\n",
       " 'tax_amount': 624.24,\n",
       " 'total_amount': 1924.74}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data[0].asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9347f5a-2c8e-442e-9909-2fba9096a7b5",
   "metadata": {},
   "source": [
    "## Función groupBy y función de agregación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05147915-a0e7-4f8e-8d64-98018f59e31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+-------------+\n",
      "| company_name|product_name|revenue_sales|\n",
      "+-------------+------------+-------------+\n",
      "|         Audi|          A4|          450|\n",
      "|Mercedes Benz|     G Class|         1200|\n",
      "|          BMW|          X1|          425|\n",
      "|     Mahindra|     XUV 500|          850|\n",
      "+-------------+------------+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkGroupBy&Agg').getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./datos/company_product_revenue.csv\",header=True,inferSchema=True)\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7971892b-6001-4a00-8c1d-a049f9d6863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue_sales per company\n",
      "+-------------+------------------+\n",
      "| company_name|sum(revenue_sales)|\n",
      "+-------------+------------------+\n",
      "|          Kia|              1140|\n",
      "|         Audi|              2275|\n",
      "|     Mahindra|              1640|\n",
      "|          BMW|              1975|\n",
      "|Mercedes Benz|              2570|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total revenue_sales per company\")\n",
    "df.groupBy(\"company_name\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "758dae1a-63a9-4ab3-a299-78c83e53b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total revenue_sales for the entire data\n",
      "+------------------+\n",
      "|sum(revenue_sales)|\n",
      "+------------------+\n",
      "|              9600|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total revenue_sales for the entire data\")\n",
    "df.agg({\"revenue_sales\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac667385-f33d-4e61-abef-d6a8b74aefb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max revenue_sales value per company\n",
      "+-------------+------------------+\n",
      "| company_name|max(revenue_sales)|\n",
      "+-------------+------------------+\n",
      "|          Kia|               690|\n",
      "|         Audi|               725|\n",
      "|     Mahindra|               850|\n",
      "|          BMW|               850|\n",
      "|Mercedes Benz|              1200|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Max revenue_sales value per company\")\n",
    "df.groupBy(\"company_name\").agg({\"revenue_sales\":\"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05a7c350-f7f5-413a-bde2-4978dad910df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordering the data based on the revenue_sales in ascending order\n",
      "+-------------+------------+-------------+\n",
      "| company_name|product_name|revenue_sales|\n",
      "+-------------+------------+-------------+\n",
      "|          BMW|          X1|          425|\n",
      "|         Audi|          A4|          450|\n",
      "|          Kia|    Carnival|          450|\n",
      "|Mercedes Benz|     C Class|          470|\n",
      "|         Audi|          Q7|          500|\n",
      "+-------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ordering the data based on the revenue_sales in ascending order\")\n",
    "df.orderBy(\"revenue_sales\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21179a3f-720a-489b-a914-4918b0fd0c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordering the data based on the revenue_sales in descending order\n",
      "+-------------+------------+-------------+\n",
      "| company_name|product_name|revenue_sales|\n",
      "+-------------+------------+-------------+\n",
      "|Mercedes Benz|     G Class|         1200|\n",
      "|Mercedes Benz|         GLS|          900|\n",
      "|     Mahindra|     XUV 500|          850|\n",
      "|          BMW|          X5|          850|\n",
      "|     Mahindra|     XUV 300|          790|\n",
      "+-------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Ordering the data based on the revenue_sales in descending order\")\n",
    "df.orderBy(df[\"revenue_sales\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a937e-df81-459c-ae55-a52d147de265",
   "metadata": {},
   "source": [
    "## Otras funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fde9541-46dd-4002-b971-982b8b14680e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Mean Revenue Sales|\n",
      "+------------------+\n",
      "| 685.7142857142857|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkInbuiltFunctions\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import mean,avg,format_number \n",
    "\n",
    "df = spark.read.csv(\"./datos/company_product_revenue.csv\",header=True,inferSchema =True)\n",
    "\n",
    "df.select(mean(\"revenue_sales\").alias(\"Mean Revenue Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3cafee4-b8a0-4cd5-84b9-68847d54fb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|Average Revenue Sales|\n",
      "+---------------------+\n",
      "|    685.7142857142857|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_avg = df.select(avg(\"revenue_sales\").alias(\"Average Revenue Sales\")) \n",
    "result_avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eeaad3b3-237a-4cd5-8946-a6b4ec932181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685.7142857142857"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_avg.head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "98712bb4-6ad7-46f7-a230-a956c9dc5f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Formatted Average|\n",
      "+-----------------+\n",
      "|           685.71|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_avg.select(format_number(\"Average revenue Sales\",2).alias(\"Formatted Average\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbfc269-5837-4f61-b27d-a645fdcbfd38",
   "metadata": {},
   "source": [
    "## Resolver datos faltantes en los dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0cc1b1ab-11e7-44ca-b394-12577d5f85b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkMisingData\").getOrCreate()\n",
    "df = spark.read.csv(\"./datos/employee_data.csv\",header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6284b79-6077-4821-9ec4-eb7b35f1ae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after dropping the rows having null values\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after dropping the rows having null values\")\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a238fa5-e83b-4b94-975f-73f4b003f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after dropping the rows having atleast 4 non-null values\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|employee_id|employee_name|age|  location|hours|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "|       G001|       Pichai| 47|California|   14|\n",
      "|       M002|         Bill| 64|Washington| null|\n",
      "+-----------+-------------+---+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after dropping the rows having atleast 4 non-null values\")\n",
    "df.na.drop(thresh=4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c631578-8459-4dd7-a33f-1346196bcb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after dropping the rows having null values in hours column\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after dropping the rows having null values in hours column\")\n",
    "df.na.drop(subset=\"hours\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "30873494-5be5-4586-af35-7dd417e1e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after filling the rows having null values in hours column\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington|   12|\n",
      "|       A003|         Jeff|  56|      null|   12|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after filling the rows having null values in hours column\")\n",
    "df.na.fill(12,subset=\"hours\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afc46b9d-0ff4-4a77-b03a-f1b67311e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after filling the rows having null values in hours column with calculated mean value\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Pichai|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington|   13|\n",
      "|       A003|         Jeff|  56|      null|   13|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean \n",
    "mean_value = df.select(mean(\"hours\")).collect()[0][0]\n",
    "\n",
    "print(\"Data after filling the rows having null values in hours column with calculated mean value\")\n",
    "df.na.fill(mean_value,subset=\"hours\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a27bd673-5352-4c4c-b14a-2b5f7275163f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after replacing a specific rows value in employee_name column\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|employee_id|employee_name| age|  location|hours|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "|       G001|       Sundar|  47|California|   14|\n",
      "|       M002|         Bill|  64|Washington| null|\n",
      "|       A003|         Jeff|  56|      null| null|\n",
      "|       A004|         null|null|      null|   12|\n",
      "+-----------+-------------+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after replacing a specific rows value in employee_name column\")\n",
    "df.na.replace(\"Pichai\",\"Sundar\",subset=\"employee_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83386d-09eb-471f-b850-319dae692534",
   "metadata": {},
   "source": [
    "## Datos fecha y hora en Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "226c4f41-688e-4273-865b-508b535ec963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+--------+----------+------------+\n",
      "|      date|item_name|item_price|quantity|tax_amount|total_amount|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "|11-10-2018|     Beer|     110.5|       2|     53.04|      163.54|\n",
      "|14-02-2018|   Whisky|    1250.0|       1|     300.0|      1550.0|\n",
      "|23-03-2020|   Whisky|    1300.5|       2|    624.24|     1924.74|\n",
      "+----------+---------+----------+--------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkDateTime\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"./datos/items_bought.csv\",header=True,inferSchema=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "09ad0789-3396-4de5-838e-8425e3c46a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema with date as string datatype\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- item_price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- tax_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Schema with date as string datatype\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b53dc01-d0c9-44cd-bb02-f86db9fc30e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema with date column string datatype converted to date datatype\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- item_price: double (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- tax_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- formatted_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date \n",
    "updated_df = df.withColumn('formatted_date',to_date(unix_timestamp(df['date'],'dd-MM-yyyy').cast('timestamp')))\n",
    "\n",
    "print(\"Schema with date column string datatype converted to date datatype\")\n",
    "updated_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c959654-67e2-43c8-b26c-6eb0620e766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after dropping the date column which was of string type\n",
      "+---------+----------+--------+----------+------------+--------------+\n",
      "|item_name|item_price|quantity|tax_amount|total_amount|formatted_date|\n",
      "+---------+----------+--------+----------+------------+--------------+\n",
      "|     Beer|     110.5|       2|     53.04|      163.54|    2018-10-11|\n",
      "|   Whisky|    1250.0|       1|     300.0|      1550.0|    2018-02-14|\n",
      "+---------+----------+--------+----------+------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Data after dropping the date column which was of string type\")\n",
    "updated_df=updated_df.drop(\"date\") #dropping the date column with string type\n",
    "updated_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "388d91d0-cd49-4453-a4a6-76d50aa09669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extraction from dates\n",
      "+---------+-----------+----------+-----+----+\n",
      "|item_name|week_number|day_number|month|year|\n",
      "+---------+-----------+----------+-----+----+\n",
      "|     Beer|         41|        11|   10|2018|\n",
      "|   Whisky|          7|        14|    2|2018|\n",
      "|   Whisky|         13|        23|    3|2020|\n",
      "+---------+-----------+----------+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import weekofyear, dayofmonth,month,year,date_format \n",
    "\n",
    "print(\"Data Extraction from dates\")\n",
    "final_df = updated_df.select(updated_df[\"item_name\"],\n",
    "weekofyear(updated_df[\"formatted_date\"]).alias(\"week_number\"),\n",
    "dayofmonth(updated_df[\"formatted_date\"]).alias(\"day_number\"),\n",
    "month(updated_df[\"formatted_date\"]).alias(\"month\"),\n",
    "year(updated_df[\"formatted_date\"]).alias(\"year\"))\n",
    "final_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5fc5b3c-723b-4ac1-8f7a-646fa036bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------------------+\n",
      "|item_name|date_format(formatted_date, MM/dd/yyyy)|\n",
      "+---------+---------------------------------------+\n",
      "|     Beer|                             10/11/2018|\n",
      "|   Whisky|                             02/14/2018|\n",
      "+---------+---------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convertir el tipo de fecha a una cadena de formato de fecha diferente\n",
    "date_string_value = updated_df.select(df[\"item_name\"],date_format(updated_df[\"formatted_date\"],'MM/dd/yyyy')) \n",
    "date_string_value.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66daf91a-155f-42a2-9412-9b1beed4641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usecase - Total amount of items purchased in that particular year\n",
      "+----+-----------------+\n",
      "|year|Total Expenditure|\n",
      "+----+-----------------+\n",
      "|2018|             6054|\n",
      "|2019|             4038|\n",
      "|2020|             8080|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Usecase - Total amount of items purchased in that particular year\")\n",
    "final_format=final_df.groupBy(\"year\").sum().select([\"year\",\"sum(year)\"])\n",
    "final_format.withColumnRenamed(\"sum(year)\",\"Total Expenditure\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
